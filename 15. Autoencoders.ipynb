{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Autoencoders in Computer Vision - Jupyter Notebook Tutorial**\n",
        "\n",
        "# Section 1: Introduction to Autoencoders\n",
        "\n",
        "\n",
        "NOTES:\n",
        "Autoencoders are neural networks designed to learn compressed representations of data.\n",
        "They work by encoding the input into a latent space and then decoding it back to reconstruct the input.\n",
        "\n",
        "This is useful in scenarios where labeled data is scarce but we still want to learn meaningful features.\n",
        "The learned features (latent vectors) are useful for:\n",
        "- Image compression\n",
        "- Noise reduction (denoising autoencoders)\n",
        "- Anomaly detection (reconstruction error)\n",
        "- Pretraining for classification or generative models\n",
        "- Transfer learning\n"
      ],
      "metadata": {
        "id": "xe_J1YFCOBcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Setup and Imports"
      ],
      "metadata": {
        "id": "vxOVlI62OR1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Byyw-rPODNu",
        "outputId": "6e37092b-b627-439e-f9a7-272156546c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: Load Dataset (MNIST)"
      ],
      "metadata": {
        "id": "lfOGsj1SOal6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_data, val_data = random_split(dataset, [50000, 10000])\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUtE8gEIObH6",
        "outputId": "e7703fde-c179-4bd1-ab26-b7be9e5c1c98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.51MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 160kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.52MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.94MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Define Basic Autoencoder"
      ],
      "metadata": {
        "id": "KkXc0neROfwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28*28, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 28*28),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Unflatten(1, (1, 28, 28))\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        out = self.decoder(z)\n",
        "        return out\n",
        "\n",
        "model = Autoencoder().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "QkXMwLXeOgS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5: Training Autoencoder"
      ],
      "metadata": {
        "id": "JputTLV_OjaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    for images, _ in train_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, images)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oIAX5dkOk7K",
        "outputId": "a8ce7e9b-01e7-4e69-af15-9c88287cbd3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.0189\n",
            "Epoch 2/5, Loss: 0.0109\n",
            "Epoch 3/5, Loss: 0.0077\n",
            "Epoch 4/5, Loss: 0.0091\n",
            "Epoch 5/5, Loss: 0.0095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 6: Visualize Reconstruction"
      ],
      "metadata": {
        "id": "ozEZG8qoOnnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_reconstruction():\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, _ in train_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            break\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2)\n",
        "    axes[0].imshow(images[0].cpu().squeeze(), cmap='gray')\n",
        "    axes[0].set_title(\"Original\")\n",
        "    axes[1].imshow(outputs[0].cpu().squeeze(), cmap='gray')\n",
        "    axes[1].set_title(\"Reconstructed\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "gE_DgJSpOpiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " After visualizing the reconstructed images, the goal is to evaluate how well the model learned the compressed representation.\n",
        "    If the reconstruction is good, it means the encoder has captured the essential information.\n",
        "\n",
        "    These latent features can now be reused:\n",
        "    - For clustering similar images\n",
        "    - As inputs to a classifier\n",
        "    - As building blocks for generative models like Variational Autoencoders or GANs"
      ],
      "metadata": {
        "id": "LXgqGlBHOs6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_reconstruction()"
      ],
      "metadata": {
        "id": "M-RLdMLLO9vG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 7: Classification Using Latent Features\n",
        "\n",
        "We now use the encoder's output (latent vector) to train a simple classifier.\n",
        "This demonstrates the power of unsupervised representation learning."
      ],
      "metadata": {
        "id": "w5wVQQjqO_zL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LatentClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LatentClassifier, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.fc(z)\n",
        "\n",
        "classifier = LatentClassifier().to(device)\n",
        "clf_criterion = nn.CrossEntropyLoss()\n",
        "clf_optimizer = optim.Adam(classifier.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "utdaSNzNPAUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the classifier using frozen encoder"
      ],
      "metadata": {
        "id": "7FzNjbT7POHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "JENDg-RhPPw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification training loop"
      ],
      "metadata": {
        "id": "p6-Odc99PQ0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    classifier.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        with torch.no_grad():\n",
        "            z = model.encoder(images)\n",
        "        preds = classifier(z)\n",
        "        loss = clf_criterion(preds, labels)\n",
        "\n",
        "        clf_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clf_optimizer.step()\n",
        "\n",
        "    print(f\"[Classifier] Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "QyyP7ExXPTbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate classifier"
      ],
      "metadata": {
        "id": "4ZU4HqicPV03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct, total = 0, 0\n",
        "classifier.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        z = model.encoder(images)\n",
        "        preds = classifier(z)\n",
        "        predicted = preds.argmax(1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "print(f\"Validation Accuracy using latent features: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "QPzu4E9fPXQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Notes:\n",
        "\n",
        "We explored basic and masked autoencoders in this notebook.\n",
        "They enable learning rich visual representations without any labels.\n",
        "\n",
        "By training to reconstruct input images or masked portions,\n",
        "autoencoders learn **meaningful features** that capture underlying structure in the data.\n",
        "These features can be used as a foundation for other computer vision tasks:\n",
        "- Classification (as demonstrated)\n",
        "- Clustering\n",
        "- Generative Modeling (e.g. VAEs, GANs)"
      ],
      "metadata": {
        "id": "xCNOKZHcPiOI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IhP_1n72zFM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing the Latent Space of Autoencoders\n",
        "\n",
        "This section will visualize Autoencoder’s latent space trained on MNIST dataset. We will try to develop some intuition about the gaps that prevent Autoencoders from being generative in nature."
      ],
      "metadata": {
        "id": "vDV9aZLBzFbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "imgs_visualize = 5000\n",
        "figsize = 10\n",
        "\n",
        "# Convert val_data (subset of MNIST) to tensors and stack\n",
        "val_images = torch.stack([item[0] for item in val_data])  # shape [10000, 1, 28, 28]\n",
        "val_labels = torch.tensor([item[1] for item in val_data])\n",
        "\n",
        "# Randomly sample imgs_visualize indices from val set\n",
        "indices = np.random.choice(len(val_images), imgs_visualize, replace=False)\n",
        "images_sample = val_images[indices].to(device)\n",
        "\n",
        "# Get latent embeddings (encoder output)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    latent_vectors = model.encoder(images_sample)  # shape [imgs_visualize, 64]\n",
        "\n",
        "# Reduce to 2D using PCA\n",
        "pca = PCA(n_components=2)\n",
        "latent_2d = pca.fit_transform(latent_vectors.cpu().numpy())\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(figsize, figsize))\n",
        "plt.scatter(latent_2d[:, 0], latent_2d[:, 1], alpha=0.5, s=2)\n",
        "plt.xlabel(\"Dimension-1\", size=20)\n",
        "plt.ylabel(\"Dimension-2\", size=20)\n",
        "plt.xticks(size=15)\n",
        "plt.yticks(size=15)\n",
        "plt.title(\"Projection of 2D Latent-Space (MNIST)\", size=20)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gE7UL5fUzGLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we will:\n",
        "- Randomly sample points in the latent space (from a normal distribution),\n",
        "\n",
        "- Feed those points into the trained decoder,\n",
        "\n",
        "- Visualize the reconstructed images.\n",
        "\n",
        "This will demonstrate that the vanilla autoencoder latent space is irregular and unstructured, so random latent vectors produce meaningless/noisy images."
      ],
      "metadata": {
        "id": "VAdvyJi8zK1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Number of random latent samples to generate\n",
        "num_samples = 10\n",
        "\n",
        "# Random latent vectors sampled from standard normal distribution (mean=0, std=1)\n",
        "random_latents = torch.randn(num_samples, 64).to(device)  # same latent size as encoder output\n",
        "\n",
        "# Decode random latent vectors\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    generated_images = model.decoder(random_latents)  # output shape [num_samples, 1, 28, 28]\n",
        "\n",
        "# Visualize generated images\n",
        "fig, axes = plt.subplots(1, num_samples, figsize=(num_samples * 2, 2))\n",
        "for i in range(num_samples):\n",
        "    axes[i].imshow(generated_images[i].cpu().squeeze(), cmap='gray')\n",
        "    axes[i].axis('off')\n",
        "    axes[i].set_title(f'Sample {i+1}')\n",
        "\n",
        "# plt.suptitle(\"Random Images Reconstructed from Random Latent Vectors\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GI6m85APzIkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our autoencoder's latent space is not regularized, sampling from a standard normal distribution does not correspond to meaningful encoded images.\n",
        "\n",
        "The decoder tries to decode these random latent vectors but produces noisy, uninterpretable images.\n",
        "\n",
        "This illustrates the motivation for models like Variational Autoencoders (VAEs) that enforce latent space priors to enable meaningful generation."
      ],
      "metadata": {
        "id": "Eg_0XbORzOo4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ElwWtIx7zPXY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}